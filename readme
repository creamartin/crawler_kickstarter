About the crawler
- builds on scrapy-playwright
- avoids bot protection
- implements hacky caching of files between requests to save Proxy bandwidth
- crawl basic pages and automate retrieve of all posts and updates
- the comment retrieval is separate because the number of comments (N>1.000000) caused issues and required workarounds not to waste resources

Setup/Install
- playwright install chromium

<discover> splits the main query(for a category) into smaller subqueries
scrapy-crawler/crawler/spiders/discover_queries.py

<exploit> gets all the project meta-data in the subqueries
scrapy-crawler/crawler/spiders/exploit_queries.py

<retrieve> downloads all the data from the individual projects
scrapy-crawler/crawler/spiders/project_spider.py
scrapy-crawler/crawler/spiders/comment_spider.py 


cd ./scrapy/crawler (important)
run "scrapy crawl <name>"

Results
name                      #records  MB(compressed)  percentage
------------------------  --------  --------------  ----------
comments                  3393988   2328            39.41     
posts                     744678    2328            39.4      
description               39376     770             13.05     
meta                      39372     196             3.33      
rewards                   287525    147             2.49      
community_per_city        350238    50              0.86      
community_per_country     302309    39              0.68      
faqs                      69900     39              0.66      
environmentalCommitments  17979     6               0.12